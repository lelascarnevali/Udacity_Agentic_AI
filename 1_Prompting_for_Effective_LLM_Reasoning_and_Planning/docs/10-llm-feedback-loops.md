# LLM Feedback Loops

## 1. Conceito Fundamental

**Feedback Loop** √© um ciclo iterativo em que o agente:
1. gera uma sa√≠da,
2. avalia o resultado,
3. transforma a avalia√ß√£o em feedback,
4. tenta novamente com base nesse feedback.

$$
\text{Output}_{t+1} = \text{LLM}\big(\text{Task} + \text{Feedback}(\text{Output}_t)\big)
$$

> Em vez de uma √∫nica tentativa (*one-shot*), o agente melhora progressivamente at√© atingir um crit√©rio de qualidade.

---

## 2. Arquitetura do Loop

- üß† **Generator LLM**: produz a primeira vers√£o da resposta.
- üß™ **Evaluator**: mede qualidade (pode ser outro LLM, regras ou testes).
- üõ†Ô∏è **Tooling/Validators**: executa checks objetivos (testes, parser, schema).
- üßæ **Feedback Builder**: converte erros e observa√ß√µes em instru√ß√µes acion√°veis.
- üîÅ **Orchestrator**: controla n√∫mero de itera√ß√µes, crit√©rio de parada e logs.

### Fluxo (vis√£o pr√°tica)

```mermaid
flowchart LR
    A[Task] --> B[LLM gera sa√≠da]
    B --> C[Valida√ß√£o / Avalia√ß√£o]
    C -->|Aprovado| D[Resposta final]
    C -->|Reprovado| E[Construir feedback]
    E --> F[Re-prompt]
    F --> B
```

---

## 3. Fontes de Feedback

| Fonte | Como funciona | Exemplo de feedback |
|---|---|---|
| **Autoavalia√ß√£o (Self-Correction)** | O pr√≥prio LLM revisa sua resposta com crit√©rios expl√≠citos | "Tom est√° informal; reescreva com linguagem profissional." |
| **Ferramentas Externas** | O output √© executado/avaliado por ferramenta real | "Teste `test_sort_numbers_basic` falhou: esperado `[1,2,3]`, obtido `[3,2,1]`." |
| **Valida√ß√£o Program√°tica** | Regras objetivas em c√≥digo | "JSON inv√°lido: campo obrigat√≥rio `email_address` ausente." |
| **Input do Usu√°rio** | Feedback humano direto | "Inclua op√ß√µes de atividades ao ar livre no roteiro." |

---

## 4. Monitoramento: o que medir

Sem monitoramento, o loop vira tentativa e erro sem controle. Monitore:

- **Qualidade da sa√≠da**: ader√™ncia a formato, precis√£o t√©cnica, completude.
- **Taxa de erro**: quantos crit√©rios falham por itera√ß√£o.
- **Ader√™ncia ao objetivo**: percentual de requisitos j√° atendidos.
- **Converg√™ncia**: melhora real entre itera√ß√µes ou estagna√ß√£o.
- **Trace por passo (logs)**: prompt, resposta, feedback e decis√£o em cada ciclo.

### Exemplo de evolu√ß√£o esperada

| Itera√ß√£o | Testes passando | Status |
|---|---:|---|
| 1 | 0/3 | Output inicial com erros |
| 2 | 1/3 | Melhorou ap√≥s feedback |
| 3 | 2/3 | Erros restantes isolados |
| 4 | 3/3 | Crit√©rio de sucesso atingido |

---

## 5. Exemplo T√©cnico: Refinamento Iterativo de C√≥digo

### Objetivo
Gerar uma fun√ß√£o Python `sort_numbers(numbers)` que retorne **nova lista** ordenada em ordem crescente.

### Ciclo
1. LLM gera c√≥digo inicial.
2. Runner executa testes.
3. Falhas viram feedback estruturado.
4. LLM reescreve a fun√ß√£o com base nesse feedback.
5. Repetir at√© passar em todos os testes ou atingir limite de itera√ß√µes.

```python
from dataclasses import dataclass
from typing import Callable


@dataclass
class LoopResult:
    code: str
    passed: bool
    test_report: str
    iterations: int


def refine_code_with_feedback(
    task_prompt: str,
    llm_call: Callable[[str], str],
    run_tests: Callable[[str], tuple[bool, str]],
    max_iterations: int = 5,
) -> LoopResult:
    """Executa um feedback loop para refinar c√≥digo gerado por LLM."""

    prompt = task_prompt
    code = ""
    report = ""

    for i in range(1, max_iterations + 1):
        code = llm_call(prompt)
        passed, report = run_tests(code)

        if passed:
            return LoopResult(code=code, passed=True, test_report=report, iterations=i)

        prompt = (
            "Reescreva a fun√ß√£o mantendo a assinatura original e corrija os erros.\n"
            f"CODIGO_ANTERIOR:\n{code}\n\n"
            f"FEEDBACK_TESTES:\n{report}\n"
        )

    return LoopResult(code=code, passed=False, test_report=report, iterations=max_iterations)
```

> **Boas pr√°ticas de seguran√ßa:** execute c√≥digo gerado em ambiente isolado (sandbox), com limite de tempo e recursos.

---

## 6. Regras de Engenharia para Loops Confi√°veis

- Defina **crit√©rio de sucesso objetivo** antes de iniciar o loop.
- Escreva feedback em formato **espec√≠fico e acion√°vel**.
- Limite itera√ß√µes (`max_iterations`) para evitar loops infinitos.
- Prefira valida√ß√µes determin√≠sticas (testes, schema) quando poss√≠vel.
- Logue cada passo para depura√ß√£o e melhoria do workflow.

---

## 7. Key Takeaways

- Feedback loops tornam agentes LLM mais confi√°veis para tarefas complexas.
- A qualidade do sistema depende da qualidade do mecanismo de feedback.
- Monitorar o ciclo √© obrigat√≥rio para avaliar progresso e corrigir falhas de projeto.
- Itera√ß√£o bem projetada aproxima agentes de comportamento realmente ag√™ntico.
