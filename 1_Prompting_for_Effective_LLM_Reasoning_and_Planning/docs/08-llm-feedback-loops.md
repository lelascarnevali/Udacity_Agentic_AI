# LLM Feedback Loops

## 1. Conceito Fundamental

**Feedback Loop** √© um ciclo iterativo em que o agente:
1. gera uma sa√≠da,
2. avalia o resultado,
3. transforma a avalia√ß√£o em feedback,
4. tenta novamente com base nesse feedback.

$$
\text{Output}_{t+1} = \text{LLM}\big(\text{Task} + \text{Feedback}(\text{Output}_t)\big)
$$

> Em vez de uma √∫nica tentativa (*one-shot*), o agente melhora progressivamente at√© atingir um crit√©rio de qualidade.

---

## 2. Arquitetura do Loop

- üß† **Generator LLM**: produz a primeira vers√£o da resposta.
- üß™ **Evaluator**: mede qualidade (pode ser outro LLM, regras ou testes).
- üõ†Ô∏è **Tooling/Validators**: executa checks objetivos (testes, parser, schema).
- üßæ **Feedback Builder**: converte erros e observa√ß√µes em instru√ß√µes acion√°veis.
- üîÅ **Orchestrator**: controla n√∫mero de itera√ß√µes, crit√©rio de parada e logs.

### Fluxo (vis√£o pr√°tica)

```mermaid
flowchart LR
    A[Task] --> B[LLM gera sa√≠da]
    B --> C[Valida√ß√£o / Avalia√ß√£o]
    C -->|Aprovado| D[Resposta final]
    C -->|Reprovado| E[Construir feedback]
    E --> F[Re-prompt]
    F --> B
```

---

## 3. Fontes de Feedback

| Fonte | Como funciona | Exemplo de feedback |
|---|---|---|
| **Autoavalia√ß√£o (Self-Correction)** | O pr√≥prio LLM revisa sua resposta com crit√©rios expl√≠citos | "Tom est√° informal; reescreva com linguagem profissional." |
| **Ferramentas Externas** | O output √© executado/avaliado por ferramenta real | "Teste `test_sort_numbers_basic` falhou: esperado `[1,2,3]`, obtido `[3,2,1]`." |
| **Valida√ß√£o Program√°tica** | Regras objetivas em c√≥digo | "JSON inv√°lido: campo obrigat√≥rio `email_address` ausente." |
| **Input do Usu√°rio** | Feedback humano direto | "Inclua op√ß√µes de atividades ao ar livre no roteiro." |

---

## 4. Monitoramento: o que medir

Sem monitoramento, o loop vira tentativa e erro sem controle. Monitore:

- **Qualidade da sa√≠da**: ader√™ncia a formato, precis√£o t√©cnica, completude.
- **Taxa de erro**: quantos crit√©rios falham por itera√ß√£o.
- **Ader√™ncia ao objetivo**: percentual de requisitos j√° atendidos.
- **Converg√™ncia**: melhora real entre itera√ß√µes ou estagna√ß√£o.
- **Trace por passo (logs)**: prompt, resposta, feedback e decis√£o em cada ciclo.

### Exemplo de evolu√ß√£o esperada

| Itera√ß√£o | Testes passando | Status |
|---|---:|---|
| 1 | 0/3 | Output inicial com erros |
| 2 | 1/3 | Melhorou ap√≥s feedback |
| 3 | 2/3 | Erros restantes isolados |
| 4 | 3/3 | Crit√©rio de sucesso atingido |

---

## 5. Exemplo T√©cnico: Refinamento Iterativo de C√≥digo

### Objetivo
Gerar uma fun√ß√£o Python `sort_numbers(numbers)` que retorne **nova lista** ordenada em ordem crescente.

### Ciclo
1. LLM gera c√≥digo inicial.
2. Runner executa testes.
3. Falhas viram feedback estruturado.
4. LLM reescreve a fun√ß√£o com base nesse feedback.
5. Repetir at√© passar em todos os testes ou atingir limite de itera√ß√µes.

```python
from dataclasses import dataclass
from typing import Callable


@dataclass
class LoopResult:
    code: str
    passed: bool
    test_report: str
    iterations: int


def refine_code_with_feedback(
    task_prompt: str,
    llm_call: Callable[[str], str],
    run_tests: Callable[[str], tuple[bool, str]],
    max_iterations: int = 5,
) -> LoopResult:
    """Executa um feedback loop para refinar c√≥digo gerado por LLM."""

    prompt = task_prompt
    code = ""
    report = ""

    for i in range(1, max_iterations + 1):
        code = llm_call(prompt)
        passed, report = run_tests(code)

        if passed:
            return LoopResult(code=code, passed=True, test_report=report, iterations=i)

        prompt = (
            "Reescreva a fun√ß√£o mantendo a assinatura original e corrija os erros.\n"
            f"CODIGO_ANTERIOR:\n{code}\n\n"
            f"FEEDBACK_TESTES:\n{report}\n"
        )

    return LoopResult(code=code, passed=False, test_report=report, iterations=max_iterations)
```

> **Boas pr√°ticas de seguran√ßa:** execute c√≥digo gerado em ambiente isolado (sandbox), com limite de tempo e recursos.

---

## 6. Regras de Engenharia para Loops Confi√°veis

- Defina **crit√©rio de sucesso objetivo** antes de iniciar o loop.
- Escreva feedback em formato **espec√≠fico e acion√°vel**.
- Limite itera√ß√µes (`max_iterations`) para evitar loops infinitos.
- Prefira valida√ß√µes determin√≠sticas (testes, schema) quando poss√≠vel.
- Logue cada passo para depura√ß√£o e melhoria do workflow.

---

## 7. Key Takeaways

- Feedback loops tornam agentes LLM mais confi√°veis para tarefas complexas.
- A qualidade do sistema depende da qualidade do mecanismo de feedback.
- Monitorar o ciclo √© obrigat√≥rio para avaliar progresso e corrigir falhas de projeto.
- Itera√ß√£o bem projetada aproxima agentes de comportamento realmente ag√™ntico.
 
---

## Implementing an AI That Learns From Its Mistakes

Voc√™ aprendeu que podemos guiar uma IA atrav√©s de tarefas complexas em m√∫ltiplas etapas. Mas o que acontece quando a IA comete um erro durante o processo?

Uma experi√™ncia comum ao usar IA para gera√ß√£o de c√≥digo: pedimos ao modelo um trecho e ele devolve algo quase perfeito ‚Äî mas com um pequeno bug, erro de sintaxe ou requisito n√£o atendido. O impulso imediato √© consertar o c√≥digo manualmente.

E se pud√©ssemos ensinar a pr√≥pria IA a corrigir seus erros? Essa √© a ideia central por tr√°s da implementa√ß√£o de um Loop de Feedback com LLMs.

Vamos ver na pr√°tica.

### Cen√°rio: Cart√£o de Perfil (HTML/CSS)

Precisamos que a IA gere o HTML e CSS de um cart√£o de perfil simples e estilizado.

#### Tentativa 1 ‚Äî Gera√ß√£o Inicial (com falha)

Primeiro, damos ao modelo o prompt inicial:

```python
# Prompt inicial para o cart√£o de perfil
prompt_initial = """
Voc√™ √© um desenvolvedor web. Gere o HTML e o CSS para um cart√£o de perfil de usu√°rio.
Deve conter:
- Um container com fundo cinza claro e sombra sutil.
- Um placeholder para a imagem do avatar.
- O nome e o cargo do usu√°rio abaixo do avatar.
"""

# Supomos que chamamos o LLM e obtemos `initial_code`
# initial_code = get_completion(prompt_initial)
# print(initial_code)
```

Sa√≠da prov√°vel (com falha): o modelo pode gerar c√≥digo funcional, por√©m com um defeito de design ‚Äî por exemplo, esquecer de centralizar o texto do nome e do cargo, deixando o cart√£o desalinhado.

#### Etapa 2 ‚Äî Mecanismo de Feedback

Em vez de consertar manualmente, atuamos como revisor de c√≥digo e fornecemos feedback. Em aplica√ß√µes reais, esse feedback pode vir de um linter, um teste de regress√£o visual ou, como no exerc√≠cio, de uma su√≠te de testes automatizados.

Para demonstra√ß√£o, o feedback ser√° uma descri√ß√£o em linguagem natural do problema:

```python
# Feedback descrevendo o problema da primeira vers√£o
feedback = "O c√≥digo gerado est√° bom como ponto de partida, mas apresenta um defeito de layout: o nome e o cargo n√£o est√£o centralizados no cart√£o. Corrija o CSS para centralizar o texto."
```

#### Etapa 3 ‚Äî Loop de Feedback (Prompt Corretivo)

Agora o mais importante: constru√≠mos um novo prompt que inclui o c√≥digo anterior e o feedback espec√≠fico.

```python
# Prompt corretivo que pede ao modelo revisar seu pr√≥prio c√≥digo
prompt_corrective = f"""
Voc√™ √© um desenvolvedor web. Voc√™ gerou anteriormente um c√≥digo que cont√©m um erro.
Por favor, revise o c√≥digo para corrigir o problema descrito no feedback.

Seu c√≥digo anterior:
---
<HTML_E_CSS_DA_VERSAO_INICIAL>
---

Feedback sobre seu c√≥digo:
---
{feedback}
---

Por favor, forne√ßa o HTML e o CSS completos e corrigidos.
"""

# corrected_code = get_completion(prompt_corrective)
# print(corrected_code)
```

Sa√≠da prov√°vel: o modelo retorna uma vers√£o corrigida onde, por exemplo, `text-align: center;` ou regras equivalentes foram aplicadas corretamente.

Este loop simples ‚Äî Gerar ‚Üí Avaliar ‚Üí Feedback ‚Üí Revisar ‚Äî √© a base para sistemas de IA que se auto-aperfei√ßoam.

Pr√≥ximo passo: automatizar o feedback substituindo a revis√£o manual por uma su√≠te de testes Python que atue como avaliador. Construa um loop que:

- chama o LLM para gerar c√≥digo;
- executa testes em sandbox (unit tests, valida√ß√µes de DOM/CSS, linters);
- gera `feedback` estruturado a partir dos resultados dos testes;
- reprompts o LLM com o c√≥digo + feedback;
- repete at√© `all_tests_pass` ou `max_iterations`.

Implemente esse loop num notebook ou script do reposit√≥rio, garantindo execu√ß√£o em ambiente isolado, logging de resultados e limite de itera√ß√µes.
