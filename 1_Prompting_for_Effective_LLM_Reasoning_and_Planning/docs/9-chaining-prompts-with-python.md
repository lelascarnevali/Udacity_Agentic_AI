# Encadeamento de Prompts com Python

## 1. Conceito Fundamental

**Prompt Chaining** √© a t√©cnica de quebrar tarefas complexas em etapas sequenciais, onde a sa√≠da de um prompt alimenta o pr√≥ximo. Isso aumenta a confiabilidade e a clareza dos resultados.

$$
	ext{Prompt Chaining} = (\text{Prompt}_1 \xrightarrow{LLM} \text{Output}_1) \rightarrow (\text{Prompt}_2(\text{Output}_1) \xrightarrow{LLM} \text{Output}_2) \rightarrow \cdots
$$

---

## 2. Componentes da Arquitetura

- **LLM**: Modelo de linguagem que executa cada etapa
- **Prompt**: Instru√ß√£o textual para o LLM
- **Chaining**: Encadeamento de prompts e respostas
- **Gate Check**: Valida√ß√£o autom√°tica (ex: sintaxe, schema)
- **Pydantic**: Estrutura√ß√£o e valida√ß√£o de dados

---

## 3. Comparativo: Chaining Tradicional vs Validado

|                     | Chaining Tradicional | Chaining Validado (Recomendado) |
|---------------------|---------------------|-------------------------------|
| Valida√ß√£o de sa√≠da  | ‚ùå N√£o               | ‚úÖ Sim (AST/Pydantic)          |
| Robustez            | M√©dia               | Alta                          |
| Debugging           | Manual              | Automatizado                  |
| Padroniza√ß√£o        | Baixa               | Alta                          |
| Uso em produ√ß√£o     | Arriscado           | Seguro                        |

> **Dica:** Sempre prefira o chaining validado para aplica√ß√µes reais!

---

## 4. Exemplo Pr√°tico: Data Analysis Script Generation

- **Objetivo:** Criar um script Python que leia um CSV (`input_data.csv`), calcule a m√©dia da coluna `value` e grave o resultado em `output.txt`.
- **Estrat√©gia:** Dividir o problema em etapas, usar a sa√≠da de um prompt como entrada do pr√≥ximo, e validar entre etapas com gate checks.

---

## 1. Implementando o encadeamento (prompt chaining)

O encadeamento √© basicamente sobre construir e manipular strings e fazer chamadas sequenciais para a API do LLM. A seguir est√° um exemplo minimalista de como organizar as etapas.

### Passo 1 ‚Äî Gerar o outline

Exemplo de prompt (padr√£o):

```python
# Prompt inicial para gerar um esbo√ßo
prompt_step1 = """
You are a helpful programming assistant.

I need a Python script to read a CSV file named 'input_data.csv',
calculate the average of a column named 'value', and write the
average to a new file named 'output.txt'.

Please provide a simple, step-by-step outline for this script.
"""

outline_response = get_completion(prompt_step1)
print(outline_response)
```

Exemplo de sa√≠da esperada (resumida):

1. Importar `csv`.
2. Abrir e ler `input_data.csv`.
3. Extrair a coluna `value` e converter para number.
4. Calcular a m√©dia.
5. Escrever o resultado em `output.txt`.

### Passo 2 ‚Äî Gerar o c√≥digo a partir do outline

O pr√≥ximo prompt reutiliza o texto retornado pelo primeiro passo.

```python
prompt_step2 = f"""
You are a helpful programming assistant.

Based on the following outline, please write the complete Python code for the script.
Ensure you use standard libraries and include comments.

Outline:
---
{outline_response}
---
"""

code_response = get_completion(prompt_step2)
print(code_response)
```

Exemplo de c√≥digo gerado (simplificado):

```python
import csv

def analyze_data():
    values = []
    try:
        with open('input_data.csv', 'r') as infile:
            reader = csv.DictReader(infile)
            for row in reader:
                values.append(float(row['value']))
    except FileNotFoundError:
        print("Error: input_data.csv not found.")
        return

    average = sum(values) / len(values) if values else 0

    with open('output.txt', 'w') as outfile:
        outfile.write(f"The average is: {average}")

if __name__ == '__main__':
    analyze_data()
```


## 2. üõ°Ô∏è Gate Check ‚Äî Valida√ß√£o Autom√°tica

Antes de executar c√≥digo gerado dinamicamente, use uma verifica√ß√£o de sintaxe. O m√≥dulo `ast` permite checar se o c√≥digo cont√©m erros de sintaxe sem execut√°-lo.

```python
import ast

def check_python_syntax(code: str):
    try:
        ast.parse(code)
        return True, "No syntax errors found."
    except SyntaxError as e:
        return False, f"Syntax Error: {e}"

# Uso:
# is_valid, message = check_python_syntax(code_response)
```

> **Debugging:**
> Se a checagem falhar, reencaminhe o c√≥digo e a mensagem de erro ao modelo pedindo corre√ß√£o e repita o gate check. Exemplo de prompt para autocorre√ß√£o:
>
> "Corrija o seguinte c√≥digo Python para remover o erro de sintaxe abaixo.\nErro: {mensagem_de_erro}\nC√≥digo:\n{codigo}" 

> **Cen√°rio real:**
> Se o LLM gerar c√≥digo inv√°lido, automatize o feedback e a revalida√ß√£o at√© passar no gate check.


## 3. üß∞ Pydantic ‚Äî Estruturar e Validar Sa√≠das

Para garantir outputs consistentes das respostas do LLM (por exemplo, quando pedimos JSON), use `pydantic` para declarar e validar o formato esperado.

**Vantagens:**
- ‚úÖ Valida√ß√£o autom√°tica de tipos e presen√ßa de campos
- ‚úÖ Parsing para objetos Python com acesso por atributos
- ‚úÖ Mensagens de erro claras para usar em gate checks

> **F√≥rmula:**
> $$\text{Valida√ß√£o} = \text{Pydantic}(\text{output\_json}) \xrightarrow{parse/validate} \text{objeto\_Python}$$

### Exemplo: modelos `OrderItem` e `Order` üõí

### Exemplo: modelos `OrderItem` e `Order`

```python
from pydantic import BaseModel, Field
from typing import List, Optional

class OrderItem(BaseModel):
    sku: str = Field(..., description="Stock Keeping Unit")
    quantity: int = Field(..., description="Quantidade do item")
    item_name: Optional[str] = Field(None, description="Nome do item, se dispon√≠vel")


class Order(BaseModel):
    order_id: int = Field(..., description="Identificador √∫nico do pedido")
    customer_email: Optional[str] = Field(None, description="Email do cliente")
    items: List[OrderItem] = Field(..., description="Lista de itens do pedido")
    total_amount: float = Field(..., description="Valor total do pedido")
```

Como usar em um gate check:

```python
from pydantic import ValidationError

def validate_order(json_data: dict):
    try:
        order = Order.parse_obj(json_data)
        return True, order
    except ValidationError as e:
        return False, e.json()
```

### Como incluir o modelo no prompt

Ao solicitar que o modelo gere JSON, inclua a descri√ß√£o dos campos (ou um esquema JSON) no prompt. Exemplo curto:

"""
Return a JSON object matching the following schema:
{
  "order_id": int,
  "customer_email": optional str,
  "items": [{"sku": str, "quantity": int, "item_name": optional str}],
  "total_amount": float
}
Respond only with valid JSON.
"""

Em seguida, parse a resposta do modelo e valide com Pydantic antes de prosseguir na cadeia.

## 4. Boas pr√°ticas e recomenda√ß√µes

- Sempre dividir tarefas complexas em etapas e validar entre etapas (gate checks).
- Pe√ßa ao LLM para responder em um formato estrito (JSON) quando o dado for consumido por c√≥digo.
- Use `ast` para checagem r√°pida de sintaxe e `pydantic` para valida√ß√£o de conte√∫do/estrutura.
- Mantenha prompts curtos e com instru√ß√µes claras sobre o formato de sa√≠da.

## 5. Exemplo r√°pido de fluxo completo (pseudo-code)

```python
# 1. Gerar outline
outline = get_completion(prompt_outline)

# 2. Gerar c√≥digo com base no outline
code = get_completion(prompt_code.format(outline=outline))

# 3. Gate check de sintaxe
ok, msg = check_python_syntax(code)
if not ok:
    # pedir corre√ß√£o ao modelo, repetir
    code = get_completion(prompt_fix.format(code=code, error=msg))

# 4. (Opcional) validar sa√≠das estruturadas com pydantic
# 5. Executar com seguran√ßa (em sandbox) ou revisar manualmente
```

---

**Contexto utilizado:**
- Arquivos criados/alterados: [1_Prompting_for_Effective_LLM_Reasoning_and_Planning/docs/9-chaining-prompts-with-python.md](1_Prompting_for_Effective_LLM_Reasoning_and_Planning/docs/9-chaining-prompts-with-python.md)
- Skills consultadas: nenhuma
- Mem√≥ria consultada: nenhuma
