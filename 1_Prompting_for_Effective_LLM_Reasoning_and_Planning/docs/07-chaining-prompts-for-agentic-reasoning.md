# Chaining Prompts for Agentic Reasoning

## 1) Por que o encadeamento de prompts importa

Modelos de Linguagem (LLMs) s√£o fortes em gera√ß√£o de uma √∫nica intera√ß√£o, mas t√™m dificuldades em workflows de m√∫ltiplas etapas, especialmente quando a tarefa exige dados externos (por exemplo: tempo, calend√°rio, invent√°rio, APIs de pol√≠tica).

Agentes de IA resolvem isso combinando racioc√≠nio com execu√ß√£o:

$$
	ext{Agent} = \text{LLM (reason)} + \text{Tools (act)} + \text{Orchestration (control)}
$$

### Componentes comuns de um agente

 - **LLM**: motor de racioc√≠nio.
 - **Tools**: APIs/fun√ß√µes para recupera√ß√£o e a√ß√µes.
 - **Instructions**: comportamento e restri√ß√µes de sistema.
 - **Memory**: contexto de curto prazo + hist√≥rico de longo prazo.
 - **Runtime/Orchestration Layer**: controla o loop e o uso de ferramentas.

Para alcan√ßar metas complexas, agentes decomp√µem tarefas em etapas menores e avaliam o progresso em cada passo.

```mermaid
flowchart TD
	LLM["LLM\n(reason)"] --> ORC["Orchestrator\n(control)"]
	ORC --> TOOLS["Tools\n(act)"]
	TOOLS --> ORC
	ORC --> MEMORY["Memory\n(context)"]
	MEMORY --> LLM
```

---

## 2) Encadeamento de Prompts: ideia central

O encadeamento conecta programaticamente sa√≠das e entradas entre chamadas:

$$
	ext{Output}_1 \rightarrow \text{Input}_2,\quad \text{Output}_2 \rightarrow \text{Input}_3
$$

### Exemplo: fluxo para publicar no LinkedIn

1. **Pesquisa** ‚Üí `RESPONSE_1`
2. **Resumir** usando `RESPONSE_1` ‚Üí `RESPONSE_2`
3. **Rascunho** usando `RESPONSE_2` ‚Üí `FINAL_RESPONSE`

Esse pipeline √© mais control√°vel e manuten√≠vel do que um √∫nico prompt muito grande.

| Padr√£o | Quando usar | Compromissos |
|---|---:|---|
| Chain (ReAct) | Tarefas multi-etapa que requerem ferramentas | Controle determin√≠stico, precisa de mais engenharia |
| Chain (CoT) | Racioc√≠nios complexos em uma √∫nica chamada | Simples, mas arriscado para uso de ferramentas |

---

## 3) Por que o encadeamento √© essencial para agentes

Pergunta: **"Que horas √© minha consulta odontol√≥gica amanh√£?"**

### Workflow r√≠gido (hard-coded)
1. Perguntar ao modelo se os dados do calend√°rio s√£o necess√°rios.
2. Se sim, chamar `get_calendar()`.
3. Pedir ao modelo para responder a partir da sa√≠da da ferramenta.

### Workflow agentivo (estilo ReAct)
1. `THOUGHT`: Preciso dos dados do calend√°rio.
2. `ACTION`: `get_calendar("amanha")`
3. Orquestrador retorna observa√ß√£o (`"9am"`).
4. `THOUGHT`: Agora tenho a resposta.
5. `ACTION`: `final_answer("9am")`

O encadeamento liga essas a√ß√µes, mas o encadeamento **sozinho** n√£o √© suficiente.

---

## 4) Valida√ß√£o de sa√≠da: Gate Checks

LLMs podem alucinar, falhar no formato ou ignorar instru√ß√µes. Erros em etapas iniciais podem se propagar.

Gate checks adicionam controle de qualidade entre etapas:

 - **Passou**: prosseguir.
 - **Falhou**: interromper, tentar novamente ou tentar com feedback de falha.

### Pseudoc√≥digo gen√©rico

```python
output = call_llm(prompt_step1)

if validate_output(output):
	next_input = process(output)
	call_llm(prompt_step2, next_input)
else:
	handle_error(output)
```

### Poor vs Optimized (code-first)

Poor (sem valida√ß√£o):

```python
def run_simple_chain(prompts, llm):
	responses = []
	for p in prompts:
		responses.append(llm(p))
	return responses[-1]
```

Otimizado (valida√ß√£o + retries):

```python
from typing import Callable
def chain_with_validation(prompts: list[str], llm: Callable[[str], str],
						  validator: Callable[[str], bool], retries: int = 2):
	last = None
	for p in prompts:
		attempt = 0
		while attempt <= retries:
			out = llm(p if last is None else f"{p}\n\nPrevious:\n{last}")
			if validator(out):
				last = out
				break
			attempt += 1
		else:
			raise RuntimeError("Validation failed after retries")
	return last
```

### Estrat√©gias t√≠picas de falha

1. **Parar** imediatamente (fluxos de alto risco).
2. **Repetir** com o mesmo prompt.
3. **Repetir com feedback** (incluir a raz√£o da falha explicitamente).

---

## 5) Tipos de Gate Checks

| Tipo | O que valida | Implementa√ß√£o comum |
|---|---|---|
| **Verifica√ß√£o de formato** | Forma JSON/XML, campos obrigat√≥rios, tamanho | Pydantic, valida√ß√£o de esquema, APIs de sa√≠da estruturada |
| **Verifica√ß√£o de conte√∫do** | Presen√ßa de palavras-chave, cita√ß√µes, relev√¢ncia | Regex, similaridade por embeddings, checagens por segundo LLM |
| **Verifica√ß√£o l√≥gica** | Consist√™ncia num√©rica/l√≥gica, qualidade de c√≥digo | `ast`, linters, testes unit√°rios, restri√ß√µes de pol√≠tica |

---

## 6) Caso de uso: gera√ß√£o de script para an√°lise de dados

Objetivo: gerar c√≥digo Python que leia um CSV, calcule a m√©dia de uma coluna e grave o resultado.

### Etapa 1 ‚Äî Gerar esbo√ßo

O prompt solicita um plano curto e numerado.

Gate 1 (opcional):
 - formato em lista presente,
 - verbos-chave presentes (`read`, `process`, `write`).

### Etapa 2 ‚Äî Gerar c√≥digo a partir do esbo√ßo

O prompt injeta `outline_response` da Etapa 1.

**Gate 2**:
 - validar sintaxe via `ast.parse()` (ou linter).

### Etapa 3 ‚Äî Refinar se a sintaxe falhar

Enviar o c√≥digo gerado e os detalhes do erro de sintaxe como feedback.

Reexecutar o Gate 2 com contador m√°ximo de tentativas.

### Fluxo fim-a-fim

1. Prompt 1 ‚Üí Esbo√ßo ‚Üí Gate 1
2. Prompt 2 ‚Üí C√≥digo ‚Üí Gate 2
3. Se falhar: Prompt 3 (corrigir) ‚Üí Gate 2 (loop de tentativa)

---

## 7) Implementando encadeamentos em Python

Em tempo de execu√ß√£o, encadear prompts √© gerenciar strings e chamadas sequenciais √† API.

```python
prompt_step1 = """
Voc√™ √© um assistente de programa√ß√£o prestativo.
Preciso de um script Python para ler input_data.csv,
calcular a m√©dia da coluna 'value' e gravar o resultado em output.txt.
Forne√ßa um plano simples passo a passo.
"""

outline_response = get_completion(prompt_step1)

prompt_step2 = f"""
Com base no esbo√ßo abaixo, escreva c√≥digo Python completo.
Use bibliotecas padr√£o e inclua coment√°rios.

Esbo√ßo:
---
{outline_response}
---
"""

code_response = get_completion(prompt_step2)
```

Verifica√ß√£o de sintaxe:

```python
import ast

def check_python_syntax(code: str) -> tuple[bool, str]:
	try:
		ast.parse(code)
		return True, "Sem erros de sintaxe."
	except SyntaxError as e:
		return False, f"Erro de sintaxe: {e}"
```

---

## 8) Pydantic para sa√≠das estruturadas confi√°veis

Sa√≠das em linguagem natural variam em estrutura e s√£o dif√≠ceis de automatizar com seguran√ßa.

Pydantic resolve isso com schemas expl√≠citos:

 - **Valida√ß√£o**: rejeita dados malformados ou incompletos.
 - **Parsing**: converte JSON em objetos tipados Python.

### Por que isso importa nas cadeias

Pydantic funciona como um gate check entre etapas:

$$
	ext{LLM JSON} \xrightarrow{\text{Pydantic validate}} \text{Typed Object} \rightarrow \text{Next Step}
$$

### Modelos de exemplo: `OrderItem` e `Order`

```python
from pydantic import BaseModel, Field
from typing import List, Optional


class OrderItem(BaseModel):
	sku: str = Field(..., description="Stock Keeping Unit")
	quantity: int = Field(..., description="Quantidade pedida")
	item_name: Optional[str] = Field(None, description="Nome do item, se dispon√≠vel")


class Order(BaseModel):
	order_id: int = Field(..., description="Identificador √∫nico do pedido")
	customer_email: Optional[str] = Field(None, description="Email do cliente")
	items: List[OrderItem] = Field(..., description="Itens do pedido")
	total_amount: float = Field(..., description="Valor total do pedido")
```

### Gate de valida√ß√£o

```python
from pydantic import ValidationError

def validate_order_payload(payload: dict) -> tuple[bool, str]:
	try:
		Order.model_validate(payload)
		return True, "Payload v√°lido"
	except ValidationError as e:
		return False, e.json()
```

---

## 9) Regras pr√°ticas de projeto

1. Divida tarefas grandes em prompts espec√≠ficos por etapa.
2. Defina crit√©rios expl√≠citos de sucesso para cada etapa.
3. Adicione gate checks determin√≠sticos quando poss√≠vel.
4. Limite tentativas para evitar loops infinitos.
5. Registre prompt/sa√≠da/valida√ß√£o a cada itera√ß√£o.
6. Use sa√≠das estruturadas + Pydantic para confiabilidade m√°quina-a-m√°quina.

---

## 10) Recapitula√ß√£o

 - A decomposi√ß√£o de tarefas melhora a qualidade do racioc√≠nio.
 - O encadeamento de prompts cria um pipeline program√°vel.
 - Gate checks evitam propaga√ß√£o de erros.
 - Pydantic torna as sa√≠das de agentes mais robustas e interoper√°veis.

Esses padr√µes formam a base de workflows agentivos confi√°veis em produ√ß√£o.

---

## üß™ Exerc√≠cios Pr√°ticos

Para aplicar os conceitos deste t√≥pico na pr√°tica, consulte:

- üìì [Lesson 4: Automated Claim Triage ‚Äî Encadeamento de Prompts](../exercises/07-lesson-4-chaining-prompts-for-agentic-reasoning.ipynb) ‚Äî pipeline de 3 est√°gios (extra√ß√£o ‚Üí avalia√ß√£o de severidade ‚Üí roteamento) com gate checks via Pydantic e valida√ß√£o estruturada

---

**T√≥pico anterior:** [Refinamento de Instru√ß√µes de Prompt e Aplica√ß√£o](06-prompt-instruction-refinement-and-application.md)
**Pr√≥ximo t√≥pico:** [Loops de Feedback para LLMs](08-llm-feedback-loops.md)