# PadrÃµes de Agentic Workflows: Evaluator-Optimizer

---

## 1. Conceito Fundamental

$$
\text{Evaluator-Optimizer} = \text{GeraÃ§Ã£o} + \text{AvaliaÃ§Ã£o} + \text{Refinamento Iterativo}
$$

**DefiniÃ§Ã£o tÃ©cnica:** O padrÃ£o Evaluator-Optimizer Ã© um workflow agÃªntico de ciclo fechado no qual um agente **Optimizer** (gerador) produz uma saÃ­da e um agente **Evaluator** (revisor) a avalia contra critÃ©rios predefinidos, fornecendo feedback acionÃ¡vel. O Optimizer entÃ£o refina a saÃ­da com base nesse feedback, e o ciclo se repete atÃ© que os critÃ©rios de qualidade sejam atingidos ou uma condiÃ§Ã£o de parada seja satisfeita.

> **Analogia:** Imagine um escritor (Optimizer) que entrega seu rascunho a um editor exigente (Evaluator). O editor marca problemas especÃ­ficos e devolve com instruÃ§Ãµes claras. O escritor revisa e re-submete. Esse ciclo continua atÃ© que o editor aprove a publicaÃ§Ã£o.

**Quando usar Evaluator-Optimizer?**

| CritÃ©rio | RecomendaÃ§Ã£o |
| :--- | :--- |
| Tarefa exige **alta qualidade e precisÃ£o** (relatÃ³rios, cÃ³digo, planos) | âœ… Ideal para Evaluator-Optimizer |
| Output deve **atender a critÃ©rios complexos ou regulatÃ³rios** | âœ… Use critÃ©rios de avaliaÃ§Ã£o explÃ­citos |
| Qualidade "suficiente" no primeiro rascunho Ã© aceitÃ¡vel | âŒ Prefira Prompt Chaining simples |
| Sub-tarefas sÃ£o **independentes** entre si | âŒ Prefira ParalelizaÃ§Ã£o |

> **Regra de Ouro â€” PrÃ©-requisito absoluto:** Defina critÃ©rios de avaliaÃ§Ã£o **claros, especÃ­ficos e mensurÃ¡veis** antes de iniciar o ciclo. Sem critÃ©rios bem definidos, o Evaluator nÃ£o tem base para fornecer feedback Ãºtil.

---

## 2. Arquitetura & Componentes

$$
\text{Tarefa} \xrightarrow{\text{Optimizer}} \text{Output}_n \xrightarrow{\text{Evaluator}} \begin{cases} \text{Aprovado} \rightarrow \text{Resultado Final} \\ \text{Feedback} \rightarrow \text{Optimizer (nova iteraÃ§Ã£o)} \end{cases}
$$

### Fluxo do PadrÃ£o Evaluator-Optimizer

```mermaid
flowchart TD
    A["ğŸ“¥ Tarefa Original<br/>+ CritÃ©rios de AvaliaÃ§Ã£o"] --> B["ğŸ¤– Optimizer<br/>(Gera/Refina Output)"]
    B --> C["ğŸ“„ Output (Tentativa N)"]
    C --> D["ğŸ” Evaluator<br/>(Avalia contra CritÃ©rios)"]
    D --> E{{"Aprovado?"}}
    E -->|"âœ… Sim"| F["ğŸ“¤ Resultado Final<br/>(Qualidade Validada)"]
    E -->|"âŒ NÃ£o"| G["ğŸ“ Feedback AcionÃ¡vel"]
    G --> B
```

### Componentes-Chave

| Componente | Papel | DescriÃ§Ã£o |
| :--- | :--- | :--- |
| ğŸ“¥ **Tarefa Original** | Entrada do Sistema | Problema a ser resolvido + critÃ©rios de sucesso |
| ğŸ¤– **Optimizer** | Gerador / Refinador | Produz o output inicial e o refina com base no feedback recebido |
| ğŸ“„ **Output** | Artefato IntermediÃ¡rio | Rascunho gerado em cada iteraÃ§Ã£o |
| ğŸ” **Evaluator** | Revisor / CrÃ­tico | Avalia o output contra critÃ©rios predefinidos e produz feedback |
| ğŸ“ **Feedback** | Canal de Melhoria | InstruÃ§Ãµes especÃ­ficas e construtivas para o Optimizer |
| ğŸ“¤ **Resultado Final** | SaÃ­da Validada | Output que atingiu os critÃ©rios de qualidade exigidos |

---

## 3. TrÃªs Elementos Essenciais do Ciclo

O poder do Evaluator-Optimizer depende de trÃªs elementos bem definidos:

### 3.1 CritÃ©rios de AvaliaÃ§Ã£o Claros (Clear Evaluation Criteria)

**O que sÃ£o:** Os padrÃµes objetivos contra os quais o Evaluator avalia o output.

**CaracterÃ­sticas de bons critÃ©rios:**

| CaracterÃ­stica | DescriÃ§Ã£o | Exemplo |
| :--- | :--- | :--- |
| **EspecÃ­ficos** | Indicam exatamente o que verificar | "O relatÃ³rio deve conter projeÃ§Ãµes de receita para 3 anos" |
| **MensurÃ¡veis** | Permitem avaliaÃ§Ã£o objetiva | "Nenhuma afirmaÃ§Ã£o especulativa sem dados de suporte" |
| **NÃ£o ambÃ­guos** | Uma Ãºnica interpretaÃ§Ã£o possÃ­vel | "Linguagem formal, sem gÃ­rias ou coloquialismos" |

> **Regra:** CritÃ©rios vagos ("seja bom") produzem feedback vago. CritÃ©rios precisos ("use dados quantitativos para toda projeÃ§Ã£o financeira") produzem refinamento direcionado.

---

### 3.2 Feedback AcionÃ¡vel (Actionable Feedback)

**O que Ã©:** As instruÃ§Ãµes especÃ­ficas que o Evaluator fornece ao Optimizer para guiar o refinamento.

| Feedback Ruim âŒ | Feedback AcionÃ¡vel âœ… |
| :--- | :--- |
| "Melhore o relatÃ³rio" | "O parÃ¡grafo 3 faz uma projeÃ§Ã£o especulativa. Substitua por dados do Q3 2025." |
| "Torne mais profissional" | "Remova os 4 adjetivos superlativos e substitua por mÃ©tricas quantitativas." |
| "EstÃ¡ quase bom" | "O critÃ©rio 'linguagem formal' foi atingido. Falta atender: projeÃ§Ãµes com dados de suporte (parÃ¡grafo 2) e disclosure de riscos (ausente)." |

**Estrutura recomendada do feedback:**
1. **Status** por critÃ©rio (aprovado/reprovado)
2. **LocalizaÃ§Ã£o** do problema (onde no output)
3. **InstruÃ§Ã£o de correÃ§Ã£o** (o que fazer para resolver)

---

### 3.3 CondiÃ§Ãµes de Parada (Stopping Conditions)

**Por que sÃ£o essenciais:** Um loop iterativo sem condiÃ§Ã£o de saÃ­da pode executar indefinidamente.

| CondiÃ§Ã£o de Parada | Quando Usar |
| :--- | :--- |
| âœ… **Todos os critÃ©rios atendidos** | CenÃ¡rio ideal â€” o output atingiu a qualidade exigida |
| ğŸ”¢ **NÃºmero mÃ¡ximo de iteraÃ§Ãµes** | Safety net â€” impede loops infinitos (ex: mÃ¡x. 5 tentativas) |
| ğŸ“‰ **Retornos decrescentes** | Melhoria entre iteraÃ§Ãµes Ã© negligÃ­vel â€” parar e aceitar |

```mermaid
flowchart LR
    A["IteraÃ§Ã£o 1<br/>Score: 4/10"] --> B["IteraÃ§Ã£o 2<br/>Score: 7/10"]
    B --> C["IteraÃ§Ã£o 3<br/>Score: 9/10"]
    C --> D["âœ… Aprovado<br/>(â‰¥ 9/10)"]

    style A fill:#ffcccc
    style B fill:#fff3cd
    style C fill:#d4edda
    style D fill:#28a745,color:#fff
```

---

## 4. ImplementaÃ§Ã£o em Python: Financial Report Generator

O exemplo a seguir demonstra o padrÃ£o Evaluator-Optimizer com um gerador de relatÃ³rios financeiros verificado por um agente de compliance.

### VisÃ£o Geral da Arquitetura

```mermaid
flowchart TD
    A["ğŸ“¥ Tarefa:<br/>Gerar RelatÃ³rio Financeiro"] --> B["ğŸ¤– FinancialReportAgent<br/>(Optimizer, temp=0.5)"]
    B --> C["ğŸ“„ RelatÃ³rio<br/>(Tentativa N)"]
    C --> D["ğŸ” ComplianceAgent<br/>(Evaluator, temp=0.0)"]
    D --> E{{"Compliance<br/>Aprovado?"}}
    E -->|"âœ… Approved"| F["ğŸ“¤ RelatÃ³rio Final"]
    E -->|"âŒ Not Approved<br/>+ Feedback"| G["ğŸ“ Feedback<br/>â†’ prÃ³ximo prompt"]
    G --> B
```

### Pattern: Evaluator-Optimizer com Compliance

```python
from typing import Callable

# Maximum number of refinement iterations (stopping condition)
MAX_ITERATIONS = 5


def evaluator_optimizer_loop(
    task: str,
    optimizer_fn: Callable[[str, str], str],
    evaluator_fn: Callable[[str], tuple[bool, str]],
    max_iterations: int = MAX_ITERATIONS,
) -> tuple[str, int]:
    """
    Generic Evaluator-Optimizer loop.

    Args:
        task: The original task description.
        optimizer_fn: Function(task, feedback) -> generated output.
        evaluator_fn: Function(output) -> (approved: bool, feedback: str).
        max_iterations: Safety limit for the loop.

    Returns:
        Tuple of (final_output, iterations_used).
    """
    feedback = ""

    for attempt in range(1, max_iterations + 1):
        # Optimizer generates or refines
        output = optimizer_fn(task, feedback)
        print(f"--- Attempt #{attempt} ---")
        print(f"Output preview: {output[:200]}...")

        # Evaluator assesses against criteria
        approved, feedback = evaluator_fn(output)

        if approved:
            print(f"âœ… Approved on attempt #{attempt}")
            return output, attempt

        print(f"âŒ Not approved. Feedback: {feedback}")

    print(f"âš ï¸ Max iterations ({max_iterations}) reached.")
    return output, max_iterations
```

### ImplementaÃ§Ã£o dos Agentes

```python
def financial_report_agent(task: str, feedback: str) -> str:
    """
    Optimizer agent: generates/refines a financial report.
    Uses higher temperature (0.5) for creative generation.
    """
    prompt = f"Task: {task}"
    if feedback:
        prompt += f"\n\nPrevious feedback to address:\n{feedback}"

    return llm_call(
        system=(
            "You are a financial report writer. "
            "Produce a professional, data-driven report. "
            "Use formal language, cite sources, and avoid speculation."
        ),
        user=prompt,
        temperature=0.5,
    )


def compliance_agent(report_text: str) -> tuple[bool, str]:
    """
    Evaluator agent: checks report against compliance criteria.
    Uses temperature=0.0 for deterministic, rule-like evaluation.

    Criteria:
      1. No speculative claims without supporting data
      2. Formal, professional language throughout
      3. Risk disclosure section present
      4. All projections backed by quantitative data
    """
    evaluation = llm_call(
        system=(
            "You are a strict compliance reviewer for financial reports. "
            "Evaluate the report against these criteria:\n"
            "1. No speculative claims without data support\n"
            "2. Formal, professional language\n"
            "3. Risk disclosure section present\n"
            "4. All projections backed by quantitative data\n\n"
            "Respond with 'Approved' if all criteria are met.\n"
            "Otherwise respond with 'Not Approved' followed by "
            "specific, actionable feedback for each failed criterion."
        ),
        user=report_text,
        temperature=0.0,
    )

    approved = evaluation.strip().lower().startswith("approved")
    return approved, evaluation


# Execution
final_report, iterations = evaluator_optimizer_loop(
    task="Generate a Q4 2025 financial performance report for investors.",
    optimizer_fn=financial_report_agent,
    evaluator_fn=compliance_agent,
)
print(f"\nFinal report generated in {iterations} iteration(s).")
```

> **ObservaÃ§Ã£o sobre `temperature`:**
> - **Evaluator (temp=0.0):** DeterminÃ­stico â€” avaliaÃ§Ã£o consistente e previsÃ­vel, como um juiz aplicando regras fixas.
> - **Optimizer (temp=0.5):** Semi-criativo â€” gera conteÃºdo variado mas focado, como um escritor competente.

---

## 5. AnÃ¡lise Comparativa: PadrÃµes de Workflow

| CritÃ©rio | Prompt Chaining | Routing | ParalelizaÃ§Ã£o | **Evaluator-Optimizer** |
| :--- | :--- | :--- | :--- | :--- |
| **Estrutura** | Sequencial linear | RamificaÃ§Ã£o dinÃ¢mica | Concorrente | **Ciclo iterativo** |
| **DependÃªncia entre etapas** | Alta (output â†’ input) | Baixa (ramificaÃ§Ã£o) | Nenhuma | **Alta (feedback â†’ refinamento)** |
| **Melhoria incremental** | Nenhuma | Nenhuma | Nenhuma | **Sim (a cada iteraÃ§Ã£o)** |
| **Velocidade** | MÃ©dia | RÃ¡pida (por rota) | RÃ¡pida (paralela) | **Lenta (mÃºltiplas iteraÃ§Ãµes)** |
| **Qualidade do output** | Boa | Boa | Diversa | **Alta (refinada iterativamente)** |
| **Ideal para** | Tarefas sequenciais | Tarefas heterogÃªneas | Alto volume | **Alta precisÃ£o / compliance** |

---

## 6. Boas PrÃ¡ticas (Golden Rules)

### âœ… FaÃ§a

1. **Defina critÃ©rios antes de implementar o loop.** Sem critÃ©rios claros, o Evaluator nÃ£o tem base para avaliar.
2. **Torne o feedback acionÃ¡vel.** Instrua o Evaluator a indicar *o quÃª* estÃ¡ errado, *onde* estÃ¡ e *como* corrigir.
3. **EstabeleÃ§a um limite mÃ¡ximo de iteraÃ§Ãµes.** Previna loops infinitos com um safety net (ex: `MAX_ITERATIONS = 5`).
4. **Use `temperature` diferenciada.** Evaluator baixo (0.0) para consistÃªncia; Optimizer moderado (0.3â€“0.7) para criatividade controlada.
5. **Acumule o feedback no prompt.** Inclua o feedback da iteraÃ§Ã£o anterior no prompt do Optimizer para guiar o refinamento.

### âŒ Evite

1. **CritÃ©rios vagos.** "Melhore a qualidade" Ã© inÃºtil â€” especifique: "Toda projeÃ§Ã£o deve ter dados de suporte quantitativos."
2. **Loop sem condiÃ§Ã£o de saÃ­da.** Sempre implemente pelo menos o limite mÃ¡ximo de iteraÃ§Ãµes.
3. **Feedback genÃ©rico.** "NÃ£o aprovado, tente novamente" nÃ£o dÃ¡ ao Optimizer informaÃ§Ã£o suficiente para melhorar.
4. **Ignorar retornos decrescentes.** Se o score nÃ£o melhora entre iteraÃ§Ãµes, aceite o melhor resultado e encerre.
5. **Mesmo `temperature` para ambos os agentes.** O Evaluator precisa ser determinÃ­stico; o Optimizer precisa de alguma criatividade.

---

## 7. Armadilhas Comuns & Debugging

| Armadilha | Sintoma | SoluÃ§Ã£o |
| :--- | :--- | :--- |
| **CritÃ©rios indefinidos** | Evaluator aprova tudo ou rejeita sem justificativa clara | Defina critÃ©rios explÃ­citos e mensurÃ¡veis no system prompt |
| **Feedback nÃ£o acionÃ¡vel** | Optimizer repete os mesmos erros a cada iteraÃ§Ã£o | Reformule o prompt do Evaluator para exigir feedback com localizaÃ§Ã£o + instruÃ§Ã£o |
| **Loop infinito** | Processo nunca termina; consome recursos indefinidamente | Implemente `MAX_ITERATIONS` como safety net |
| **Retornos decrescentes** | Score estagna apÃ³s 2-3 iteraÃ§Ãµes | Adicione detecÃ§Ã£o de plateau; aceite o melhor resultado |
| **Feedback conflitante** | Optimizer tenta corrigir X mas quebra Y | Numere critÃ©rios e exija avaliaÃ§Ã£o item por item |
| **Evaluator muito permissivo** | Output de baixa qualidade Ã© aprovado na primeira tentativa | Revise e refine os critÃ©rios; use `temperature=0.0` no Evaluator |

---

## 8. O que Observar na Demo

Ao executar a demo do Financial Report Generator, foque em:

| Ponto de AtenÃ§Ã£o | O que Observar |
| :--- | :--- |
| ğŸ”¢ **Tentativas** | Cada `Attempt #N` no console representa uma iteraÃ§Ã£o do ciclo |
| ğŸ“ **Feedback do ComplianceAgent** | InstruÃ§Ãµes especÃ­ficas quando o draft nÃ£o Ã© aprovado |
| ğŸ”„ **EvoluÃ§Ã£o do prompt** | O prompt do FinancialReportAgent muda a cada tentativa (acumula feedback) |
| ğŸ“ˆ **Melhoria do relatÃ³rio** | O `report_text` se torna mais compliant a cada iteraÃ§Ã£o |
| ğŸŒ¡ï¸ **Temperature settings** | ComplianceAgent (0.0) = determinÃ­stico; FinancialReportAgent (0.5) = criativo |

---

## 9. Resumo & PrÃ³ximos Passos

VocÃª dominou o padrÃ£o **Evaluator-Optimizer**, habilitando a criaÃ§Ã£o de outputs de alta qualidade atravÃ©s de um ciclo de geraÃ§Ã£o, crÃ­tica e refinamento iterativo.

### Habilidades Desenvolvidas

âœ… **Designing Iterative AI Systems** â€” Projetar workflows onde agentes interagem em loop, construindo sobre outputs anteriores
âœ… **Crafting Evaluation Criteria** â€” Definir critÃ©rios claros, especÃ­ficos e mensurÃ¡veis para avaliaÃ§Ã£o automatizada
âœ… **Prompting for Feedback & Refinement** â€” Instruir LLMs a avaliar conteÃºdo contra critÃ©rios e a usar feedback para melhorar geraÃ§Ãµes subsequentes
âœ… **Temperature Strategy** â€” Calibrar `temperature` por papel (baixo para avaliaÃ§Ã£o determinÃ­stica, moderado para geraÃ§Ã£o criativa)
âœ… **Safety Mechanisms** â€” Implementar condiÃ§Ãµes de parada para garantir que loops iterativos sejam finitos e eficientes

---

## ğŸ§ª ExercÃ­cios PrÃ¡ticos

Para aplicar os conceitos deste tÃ³pico na prÃ¡tica, consulte:

- ğŸ [Evaluator-Optimizer â€” Demo](../exercises/7-agentic-evaluator-optimizer-demo.py) â€” demonstraÃ§Ã£o completa do padrÃ£o com FinancialReportAgent (optimizer) e ComplianceAgent (evaluator) em loop iterativo com MAX_RETRIES
- ğŸ [Evaluator-Optimizer â€” ExercÃ­cio](../exercises/7-agentic-evaluator-optimizer.py) â€” exercÃ­cio com esqueleto para implementar o padrÃ£o aplicado a geraÃ§Ã£o de receitas com mÃºltiplas restriÃ§Ãµes (gluten-free, vegan, calorias, proteÃ­na)

---

[â† TÃ³pico Anterior: PadrÃµes de Agentic Workflows: ParalelizaÃ§Ã£o](06-agentic-workflow-patterns-parallelization.md) | [PrÃ³ximo TÃ³pico: PadrÃµes de Agentic Workflows: Orchestrator-Workers â†’](08-agentic-workflow-patterns-orchestrator-workers.md)
